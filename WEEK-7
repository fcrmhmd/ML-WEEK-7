{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPXtKX8cSya65fwImCL7Tu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fcrmhmd/ML-WEEK-7/blob/main/WEEK-7\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "g0xcuY4fOyZ0",
        "outputId": "ba14f194-1755-4938-daca-5b8b619a023b"
      },
      "source": [
        "for best explain, please check:\n",
        " \n",
        "https://www.kdnuggets.com/2017/03/simple-xgboost-tutorial-iris-dataset.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-62cfa8fe5800>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for best explain, please check:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "mUfiyl2NOzxm",
        "outputId": "3f2b453c-2cd9-45c0-bd64-addf1152bb08"
      },
      "source": [
        "# Ieva Zarina, 2016, licensed under the Apache 2.0 licnese\n",
        "\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import dump_svmlight_file\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# use DMatrix for xgbosot\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# use svmlight file for xgboost\n",
        "dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\n",
        "dump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)\n",
        "dtrain_svm = xgb.DMatrix('dtrain.svm')\n",
        "dtest_svm = xgb.DMatrix('dtest.svm')\n",
        "\n",
        "# set xgboost params\n",
        "param = {\n",
        "    'max_depth': 3,  # the maximum depth of each tree\n",
        "    'eta': 0.3,  # the training step for each iteration\n",
        "    'silent': 1,  # logging mode - quiet\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
        "    'num_class': 3}  # the number of classes that exist in this datset\n",
        "num_round = 20  # the number of training iterations\n",
        "\n",
        "#------------- numpy array ------------------\n",
        "# training and testing - numpy matrices\n",
        "bst = xgb.train(param, dtrain, num_round)\n",
        "preds = bst.predict(dtest)\n",
        "\n",
        "# extracting most confident predictions\n",
        "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "print(\"Numpy array precision:\", precision_score(y_test, best_preds, average='macro'))\n",
        "\n",
        "# ------------- svm file ---------------------\n",
        "# training and testing - svm file\n",
        "bst_svm = xgb.train(param, dtrain_svm, num_round)\n",
        "preds = bst.predict(dtest_svm)\n",
        "\n",
        "# extracting most confident predictions\n",
        "best_preds_svm = [np.argmax(line) for line in preds]\n",
        "print(\"Svm file precision:\",precision_score(y_test, best_preds_svm, average='macro'))\n",
        "# --------------------------------------------\n",
        "\n",
        "# dump the models\n",
        "bst.dump_model('dump.raw.txt')\n",
        "bst_svm.dump_model('dump_svm.raw.txt')\n",
        "\n",
        "\n",
        "# save the models for later\n",
        "joblib.dump(bst, 'bst_model.pkl', compress=True)\n",
        "joblib.dump(bst_svm, 'bst_svm_model.pkl', compress=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-61bf977bbd83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_svmlight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/usr/local/lib/python3.7/dist-packages/sklearn/externals/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Xo7ur0Oz0N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGm7Qk7-Oz2x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-RfU_2Oz5R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gu3WNjOz7x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfQ3qawqOz-k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S12E4OkaO0BX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAIcspadO0D-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On9z5n6rO0G1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXfVXZnhO0Jd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBjRG3eNO0MA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1eMfQqhO0Oz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQUIBUKdO0Ro"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyqVR8L2O0U_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}